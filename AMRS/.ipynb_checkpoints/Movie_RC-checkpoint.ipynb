{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14bbfe1b-2fda-48d7-af05-89e9e82dca7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTrain a multi-label movie-genre classifier on IMDb overviews and save artifacts.\\n\\nInputs:\\n  - imdb_movies.csv  (must include columns: 'title', 'overview', 'genre')\\n\\nOutputs (saved under ./artifacts):\\n  - vectorizer.joblib         (fitted TfidfVectorizer)\\n  - mlb.joblib                (fitted MultiLabelBinarizer)\\n  - model.joblib              (fitted OneVsRest LogisticRegression)\\n  - thresholds.npy            (per-class decision thresholds)\\n  - labeled_overviews.npz     (sparse TF-IDF matrix of labeled overviews)\\n  - labeled_titles.json       (list of titles aligned with rows in the matrix)\\n  - classes.json              (list of genre class names in mlb order)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train a multi-label movie-genre classifier on IMDb overviews and save artifacts.\n",
    "\n",
    "Inputs:\n",
    "  - imdb_movies.csv  (must include columns: 'title', 'overview', 'genre')\n",
    "\n",
    "Outputs (saved under ./artifacts):\n",
    "  - vectorizer.joblib         (fitted TfidfVectorizer)\n",
    "  - mlb.joblib                (fitted MultiLabelBinarizer)\n",
    "  - model.joblib              (fitted OneVsRest LogisticRegression)\n",
    "  - thresholds.npy            (per-class decision thresholds)\n",
    "  - labeled_overviews.npz     (sparse TF-IDF matrix of labeled overviews)\n",
    "  - labeled_titles.json       (list of titles aligned with rows in the matrix)\n",
    "  - classes.json              (list of genre class names in mlb order)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d063479-6c5e-4a16-93f9-307e42e4fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "# Persistence\n",
    "import joblib\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH = \"imdb_movies.csv\"\n",
    "ARTIFACT_DIR = Path(\"artifacts\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32d1a1f-cc57-4ecb-9a67-71bf353cfab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resource: wordnet\n",
      "Downloading NLTK resource: omw-1.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/manjunathpopuri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/manjunathpopuri/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def ensure_nltk_data():\n",
    "    \"\"\"Download required NLTK packages if missing.\"\"\"\n",
    "    required = {\n",
    "        'punkt': 'tokenizers/punkt',\n",
    "        'stopwords': 'corpora/stopwords',\n",
    "        'wordnet': 'corpora/wordnet',\n",
    "        'omw-1.4': 'corpora/omw-1.4',\n",
    "    }\n",
    "    for pkg, res in required.items():\n",
    "        try:\n",
    "            nltk.data.find(res)\n",
    "        except LookupError:\n",
    "            print(f\"Downloading NLTK resource: {pkg}\")\n",
    "            nltk.download(pkg, quiet=False)\n",
    "\n",
    "ensure_nltk_data()\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "LEMM = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60ba4dc-ab66-4e2c-8a89-118ef868ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading & cleaning\n",
    "\n",
    "def load_data(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required_cols = {\"names\", \"overview\", \"genre\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\"CSV must contain columns: 'names', 'overview', 'genre'\")\n",
    "\n",
    "    unlabeled_mask = df[\"genre\"].isna()\n",
    "    labeled_df = df[~unlabeled_mask].copy()\n",
    "\n",
    "    # Normalize genres: split/comma -> list\n",
    "    labeled_df[\"genre\"] = labeled_df[\"genre\"].str.split(\",\").apply(label_preprocess)\n",
    "\n",
    "    # Basic text cleaning on overviews\n",
    "    labeled_df[\"overview\"] = labeled_df[\"overview\"].fillna(\"\").apply(preprocess_text)\n",
    "    return labeled_df\n",
    "\n",
    "\n",
    "def label_preprocess(labels):\n",
    "    out = []\n",
    "    for lab in labels:\n",
    "        lab = lab.strip().lower().replace(\"\\u00A0\", \"\")\n",
    "        if lab:\n",
    "            out.append(lab)\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    x = re.sub(r\"[^\\w\\s]\", \" \", text or \"\")\n",
    "    x = x.lower()\n",
    "    words = word_tokenize(x)\n",
    "    words = [w for w in words if w not in STOP_WORDS and w.isalpha()]\n",
    "    words = [LEMM.lemmatize(w) for w in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6e8b85-ce8f-484d-9cb9-c3109aee32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Evaluate / Save\n",
    "def tune_thresholds(y_true: np.ndarray, y_proba: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Choose per-class threshold that maximizes F1 on the PR curve.\"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    n_labels = y_true.shape[1]\n",
    "    best_thresholds = np.empty(n_labels, dtype=float)\n",
    "\n",
    "    for i in range(n_labels):\n",
    "        p, r, t = precision_recall_curve(y_true[:, i], y_proba[:, i])\n",
    "        # The last precision/recall point has no threshold\n",
    "        f1 = 2 * p * r / (p + r + 1e-12)\n",
    "        f1 = f1[:-1]\n",
    "        if t.size == 0 or np.all(np.isnan(f1)):\n",
    "            best_thresholds[i] = 0.5\n",
    "        else:\n",
    "            j = np.nanargmax(f1)\n",
    "            best_thresholds[i] = t[j]\n",
    "    return best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a401f060-ad04-4f92-994c-3fec67d88931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-F1: 0.6279\n",
      "Macro-F1: 0.5762\n",
      "\n",
      "Per-class report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "         action       0.66      0.74      0.70       842\n",
      "      adventure       0.51      0.70      0.59       564\n",
      "      animation       0.61      0.72      0.66       471\n",
      "         comedy       0.55      0.74      0.63       897\n",
      "          crime       0.50      0.73      0.60       409\n",
      "    documentary       0.65      0.52      0.57        60\n",
      "          drama       0.58      0.76      0.66      1111\n",
      "         family       0.63      0.80      0.70       433\n",
      "        fantasy       0.61      0.58      0.59       408\n",
      "        history       0.45      0.42      0.43       114\n",
      "         horror       0.67      0.69      0.68       466\n",
      "          music       0.55      0.54      0.55        94\n",
      "        mystery       0.37      0.49      0.42       258\n",
      "        romance       0.54      0.70      0.61       491\n",
      "science fiction       0.66      0.64      0.65       397\n",
      "       thriller       0.53      0.79      0.64       782\n",
      "       tv movie       0.19      0.14      0.17        69\n",
      "            war       0.56      0.49      0.52        82\n",
      "        western       0.56      0.61      0.59        36\n",
      "\n",
      "      micro avg       0.57      0.70      0.63      7984\n",
      "      macro avg       0.55      0.62      0.58      7984\n",
      "   weighted avg       0.57      0.70      0.63      7984\n",
      "    samples avg       0.58      0.72      0.61      7984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artifacts saved to ./artifacts\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10497dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df = load_data(DATA_PATH)\n",
    "    X_text = df[\"overview\"].astype(str).tolist()\n",
    "    y_raw = df[\"genre\"].tolist()\n",
    "    titles = df[\"names\"].astype(str).tolist()\n",
    "\n",
    "    # Vectorize\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=50_000,\n",
    "        min_df=2\n",
    "    )\n",
    "    X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "    # Labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y = mlb.fit_transform(y_raw)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test, titles_train, titles_test = train_test_split(\n",
    "        X, Y, titles, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    lr = LogisticRegression(\n",
    "        C=1.0,\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        solver=\"lbfgs\"\n",
    "    )\n",
    "    clf = OneVsRestClassifier(lr)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Raw probs on test\n",
    "    y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "    # Per-class threshold tuning\n",
    "    thresholds = tune_thresholds(y_test, y_proba)\n",
    "\n",
    "    # Apply thresholds\n",
    "    y_pred = (y_proba >= thresholds).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    micro = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    print(f\"\\nMicro-F1: {micro:.4f}\")\n",
    "    print(f\"Macro-F1: {macro:.4f}\\n\")\n",
    "    print(\"Per-class report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=mlb.classes_))\n",
    "\n",
    "    # Persist artifacts\n",
    "    joblib.dump(vectorizer, ARTIFACT_DIR / \"vectorizer.joblib\")\n",
    "    joblib.dump(mlb, ARTIFACT_DIR / \"mlb.joblib\")\n",
    "    joblib.dump(clf, ARTIFACT_DIR / \"model.joblib\")\n",
    "    np.save(ARTIFACT_DIR / \"thresholds.npy\", thresholds)\n",
    "\n",
    "    # Save labeled TF-IDF matrix and aligned titles for similarity search\n",
    "    sparse.save_npz(ARTIFACT_DIR / \"labeled_overviews.npz\", X)\n",
    "    with open(ARTIFACT_DIR / \"labeled_titles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(titles, f)\n",
    "    with open(ARTIFACT_DIR / \"classes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(mlb.classes_), f)\n",
    "\n",
    "    print(\"\\nArtifacts saved to ./artifacts\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a2294-acb6-40d6-9024-0875dedf935f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09742e79-8e52-4f80-a606-a508fd46fd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
